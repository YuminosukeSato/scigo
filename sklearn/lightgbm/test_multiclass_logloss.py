#!/usr/bin/env python3
"""
MulticlassLogLosså®Ÿè£…ã®æ¤œè¨¼ãƒ†ã‚¹ãƒˆ

ã“ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã¯ã€Goã§å®Ÿè£…ã•ã‚ŒãŸMulticlassLogLossãŒ
ç†è«–çš„ãªè¨ˆç®—ã¨scipyå®Ÿè£…ã¨å®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import numpy as np
import json
import os
from scipy.special import softmax, log_softmax
from typing import Tuple, Dict, Any


def stable_softmax(logits: np.ndarray) -> np.ndarray:
    """æ•°å€¤å®‰å®šæ€§ã‚’è€ƒæ…®ã—ãŸSoftmaxè¨ˆç®—"""
    # å„ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«æœ€å¤§å€¤ã‚’æ¸›ç®—
    max_logits = np.max(logits, axis=-1, keepdims=True)
    exp_logits = np.exp(logits - max_logits)
    return exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)


def calculate_gradients_and_hessians(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    å¤šã‚¯ãƒ©ã‚¹äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã®å‹¾é…ã¨ãƒ˜ã‚·ã‚¢ãƒ³ã‚’è¨ˆç®—
    
    Args:
        y_true: çœŸã®ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ« [num_samples]
        y_pred: äºˆæ¸¬ãƒ­ã‚¸ãƒƒãƒˆ [num_samples, num_classes]
        
    Returns:
        gradients: [num_samples, num_classes]
        hessians: [num_samples, num_classes]
    """
    num_samples, num_classes = y_pred.shape
    
    # Softmaxç¢ºç‡ã‚’è¨ˆç®—
    probabilities = stable_softmax(y_pred)
    
    # One-hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    y_true_onehot = np.zeros((num_samples, num_classes))
    y_true_onehot[np.arange(num_samples), y_true] = 1.0
    
    # å‹¾é…: p_k - y_k
    gradients = probabilities - y_true_onehot
    
    # ãƒ˜ã‚·ã‚¢ãƒ³ï¼ˆå¯¾è§’è¿‘ä¼¼ï¼‰: p_k * (1 - p_k)
    hessians = probabilities * (1.0 - probabilities)
    
    # æ•°å€¤å®‰å®šæ€§ã®ç¢ºä¿
    hessians = np.maximum(hessians, 1e-16)
    
    return gradients, hessians


def calculate_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """å¤šã‚¯ãƒ©ã‚¹äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã‚’è¨ˆç®—"""
    num_samples = y_pred.shape[0]
    
    # æ•°å€¤å®‰å®šæ€§ã‚’è€ƒæ…®ã—ãŸlog softmaxã‚’è¨ˆç®—
    log_probs = log_softmax(y_pred, axis=1)
    
    # çœŸã®ã‚¯ãƒ©ã‚¹ã®logç¢ºç‡ã‚’æŠ½å‡º
    true_log_probs = log_probs[np.arange(num_samples), y_true]
    
    # å¹³å‡äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±: -mean(log(p_true_class))
    return -np.mean(true_log_probs)


def test_basic_gradients_hessians():
    """åŸºæœ¬çš„ãªå‹¾é…ãƒ»ãƒ˜ã‚·ã‚¢ãƒ³è¨ˆç®—ã®ãƒ†ã‚¹ãƒˆ"""
    print("=== åŸºæœ¬çš„ãªå‹¾é…ãƒ»ãƒ˜ã‚·ã‚¢ãƒ³è¨ˆç®—ãƒ†ã‚¹ãƒˆ ===")
    
    # 3ã‚¯ãƒ©ã‚¹ã€4ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    y_true = np.array([0, 1, 2, 0])
    y_pred = np.array([
        [2.0, 0.5, -1.0],    # ã‚¯ãƒ©ã‚¹0ãŒæœ€å¤§
        [-0.5, 3.0, -1.0],   # ã‚¯ãƒ©ã‚¹1ãŒæœ€å¤§
        [0.0, -1.0, 2.5],    # ã‚¯ãƒ©ã‚¹2ãŒæœ€å¤§
        [1.8, 0.3, -0.8]     # ã‚¯ãƒ©ã‚¹0ãŒæœ€å¤§
    ])
    
    gradients, hessians = calculate_gradients_and_hessians(y_true, y_pred)
    loss = calculate_loss(y_true, y_pred)
    
    print(f"å…¥åŠ›ãƒ­ã‚¸ãƒƒãƒˆ:\n{y_pred}")
    print(f"çœŸã®ã‚¯ãƒ©ã‚¹: {y_true}")
    print(f"å‹¾é…:\n{gradients}")
    print(f"ãƒ˜ã‚·ã‚¢ãƒ³:\n{hessians}")
    print(f"æå¤±: {loss:.6f}")
    
    # åŸºæœ¬çš„ãªæ€§è³ªã‚’ãƒã‚§ãƒƒã‚¯
    # 1. å‹¾é…ã®å„è¡Œã®åˆè¨ˆã¯0ã«è¿‘ã„ï¼ˆsoftmaxåˆ¶ç´„ï¼‰
    gradient_sums = np.sum(gradients, axis=1)
    print(f"å‹¾é…è¡Œåˆè¨ˆ: {gradient_sums}")
    assert np.allclose(gradient_sums, 0.0, atol=1e-10), "å‹¾é…ã®è¡Œåˆè¨ˆãŒ0ã«ãªã£ã¦ã„ã¾ã›ã‚“"
    
    # 2. ãƒ˜ã‚·ã‚¢ãƒ³ã¯å…¨ã¦æ­£å€¤
    assert np.all(hessians > 0), "ãƒ˜ã‚·ã‚¢ãƒ³ã«éæ­£å€¤ãŒã‚ã‚Šã¾ã™"
    
    # 3. ç¢ºç‡ã®åˆè¨ˆãŒ1
    probabilities = stable_softmax(y_pred)
    prob_sums = np.sum(probabilities, axis=1)
    assert np.allclose(prob_sums, 1.0), "ç¢ºç‡ã®åˆè¨ˆãŒ1ã«ãªã£ã¦ã„ã¾ã›ã‚“"
    
    print("âœ… åŸºæœ¬ãƒ†ã‚¹ãƒˆåˆæ ¼\n")
    return {
        'y_true': y_true.tolist(),
        'y_pred': y_pred.tolist(),
        'gradients': gradients.tolist(),
        'hessians': hessians.tolist(),
        'loss': float(loss)
    }


def test_edge_cases():
    """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆ"""
    print("=== ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ ===")
    
    # æ¥µç«¯ãªãƒ­ã‚¸ãƒƒãƒˆå€¤
    y_true = np.array([0, 1])
    y_pred = np.array([
        [100.0, -50.0, -50.0],  # æ¥µã‚ã¦ç¢ºä¿¡åº¦ã®é«˜ã„äºˆæ¸¬
        [-50.0, 100.0, -50.0]   # æ¥µã‚ã¦ç¢ºä¿¡åº¦ã®é«˜ã„äºˆæ¸¬
    ])
    
    gradients, hessians = calculate_gradients_and_hessians(y_true, y_pred)
    loss = calculate_loss(y_true, y_pred)
    
    print(f"æ¥µç«¯ãªãƒ­ã‚¸ãƒƒãƒˆå€¤ã§ã®æå¤±: {loss:.6f}")
    print(f"å‹¾é…ç¯„å›²: [{np.min(gradients):.6f}, {np.max(gradients):.6f}]")
    print(f"ãƒ˜ã‚·ã‚¢ãƒ³ç¯„å›²: [{np.min(hessians):.6f}, {np.max(hessians):.6f}]")
    
    # å€¤ãŒæœ‰é™ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    assert np.all(np.isfinite(gradients)), "å‹¾é…ã«ç„¡é™å€¤ãŒã‚ã‚Šã¾ã™"
    assert np.all(np.isfinite(hessians)), "ãƒ˜ã‚·ã‚¢ãƒ³ã«ç„¡é™å€¤ãŒã‚ã‚Šã¾ã™"
    assert np.isfinite(loss), "æå¤±ãŒç„¡é™å€¤ã§ã™"
    
    print("âœ… ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆåˆæ ¼\n")
    
    return {
        'y_true': y_true.tolist(),
        'y_pred': y_pred.tolist(),
        'gradients': gradients.tolist(),
        'hessians': hessians.tolist(),
        'loss': float(loss)
    }


def test_large_dataset():
    """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ãƒ†ã‚¹ãƒˆ"""
    print("=== å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆ ===")
    
    np.random.seed(42)
    num_samples = 1000
    num_classes = 5
    
    # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ­ã‚¸ãƒƒãƒˆã¨çœŸã®ã‚¯ãƒ©ã‚¹
    y_pred = np.random.randn(num_samples, num_classes) * 2
    y_true = np.random.randint(0, num_classes, num_samples)
    
    gradients, hessians = calculate_gradients_and_hessians(y_true, y_pred)
    loss = calculate_loss(y_true, y_pred)
    
    print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {num_samples}, ã‚¯ãƒ©ã‚¹æ•°: {num_classes}")
    print(f"æå¤±: {loss:.6f}")
    print(f"å‹¾é…çµ±è¨ˆ: å¹³å‡={np.mean(gradients):.6f}, æ¨™æº–åå·®={np.std(gradients):.6f}")
    print(f"ãƒ˜ã‚·ã‚¢ãƒ³çµ±è¨ˆ: å¹³å‡={np.mean(hessians):.6f}, æ¨™æº–åå·®={np.std(hessians):.6f}")
    
    # åŸºæœ¬çš„ãªæ€§è³ªã‚’ãƒã‚§ãƒƒã‚¯
    gradient_sums = np.sum(gradients, axis=1)
    assert np.allclose(gradient_sums, 0.0, atol=1e-10), "å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å‹¾é…åˆ¶ç´„ãŒé•åã•ã‚Œã¦ã„ã¾ã™"
    
    print("âœ… å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆåˆæ ¼\n")
    
    return {
        'num_samples': num_samples,
        'num_classes': num_classes,
        'loss': float(loss),
        'gradient_mean': float(np.mean(gradients)),
        'gradient_std': float(np.std(gradients)),
        'hessian_mean': float(np.mean(hessians)),
        'hessian_std': float(np.std(hessians))
    }


def create_go_test_data():
    """Goå®Ÿè£…ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    print("=== Goå®Ÿè£…ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ ===")
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹1: åŸºæœ¬ã‚±ãƒ¼ã‚¹
    basic_case = test_basic_gradients_hessians()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹2: ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹
    edge_case = test_edge_cases()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹3: ã‚ˆã‚Šè¤‡é›‘ãªã‚±ãƒ¼ã‚¹
    np.random.seed(123)
    y_true_complex = np.random.randint(0, 4, 20)
    y_pred_complex = np.random.randn(20, 4) * 1.5
    
    gradients_complex, hessians_complex = calculate_gradients_and_hessians(y_true_complex, y_pred_complex)
    loss_complex = calculate_loss(y_true_complex, y_pred_complex)
    
    complex_case = {
        'y_true': y_true_complex.tolist(),
        'y_pred': y_pred_complex.tolist(),
        'gradients': gradients_complex.tolist(),
        'hessians': hessians_complex.tolist(),
        'loss': float(loss_complex)
    }
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆ
    test_data = {
        'basic_case': basic_case,
        'edge_case': edge_case,
        'complex_case': complex_case,
        'test_info': {
            'description': 'MulticlassLogLoss validation data for Go implementation',
            'gradient_formula': 'gradient[i,k] = p[i,k] - y[i,k] (where p=softmax(logits), y=onehot(true))',
            'hessian_formula': 'hessian[i,k] = p[i,k] * (1 - p[i,k])',
            'loss_formula': 'loss = -mean(log(p[i,true_class[i]]))'
        }
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    os.makedirs('testdata', exist_ok=True)
    with open('testdata/multiclass_logloss_test_data.json', 'w') as f:
        json.dump(test_data, f, indent=2)
    
    print(f"âœ… Goå®Ÿè£…ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: testdata/multiclass_logloss_test_data.json")
    return test_data


def main():
    """å…¨ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    print("=== MulticlassLogLosså®Ÿè£…æ¤œè¨¼ãƒ†ã‚¹ãƒˆé–‹å§‹ ===\n")
    
    try:
        # åŸºæœ¬ãƒ†ã‚¹ãƒˆ
        test_basic_gradients_hessians()
        
        # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
        test_edge_cases()
        
        # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
        test_large_dataset()
        
        # Goå®Ÿè£…ç”¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        create_go_test_data()
        
        print("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆå®Œäº†: SUCCESS")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        raise


if __name__ == "__main__":
    main()
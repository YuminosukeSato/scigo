#!/usr/bin/env python3
"""
PredictProba (ç¢ºç‡äºˆæ¸¬) æ¤œè¨¼ãƒ†ã‚¹ãƒˆ

ã“ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã¯ã€Goã§å®Ÿè£…ã•ã‚ŒãŸPredictProbaãŒ
Python LightGBMã¨åŒç­‰ã®å‹•ä½œã‚’ã™ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
ç‰¹ã«MulticlassLogLossã¨ã®é€£æºã‚’ä¸­å¿ƒã«ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚
"""

import numpy as np
import pandas as pd
import lightgbm as lgb
import json
import os
from typing import Dict, List, Tuple, Optional
from scipy.special import softmax


def create_multiclass_dataset(n_samples: int = 300, n_features: int = 6, n_classes: int = 3, seed: int = 42) -> Tuple[np.ndarray, np.ndarray]:
    """
    å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    
    Args:
        n_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°
        n_features: ç‰¹å¾´é‡æ•°
        n_classes: ã‚¯ãƒ©ã‚¹æ•°
        seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        
    Returns:
        X: ç‰¹å¾´é‡è¡Œåˆ—
        y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé…åˆ—ï¼ˆã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ï¼‰
    """
    np.random.seed(seed)
    
    # ç‰¹å¾´é‡è¡Œåˆ—ã®ç”Ÿæˆ
    X = np.random.randn(n_samples, n_features)
    
    # ã‚¯ãƒ©ã‚¹åˆ¥ã®é‡ã¿ä¿‚æ•°ã‚’è¨­å®š
    class_weights = []
    for c in range(n_classes):
        weights = np.random.randn(n_features) * 0.5
        class_weights.append(weights)
    
    # å„ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’è¨ˆç®—
    logits = np.zeros((n_samples, n_classes))
    for c in range(n_classes):
        logits[:, c] = X @ class_weights[c] + np.random.randn(n_samples) * 0.1
    
    # Softmax ã§ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’è¨ˆç®—
    probabilities = softmax(logits, axis=1)
    
    # ç¢ºç‡ã«åŸºã¥ã„ã¦ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    y = np.array([np.random.choice(n_classes, p=probabilities[i]) for i in range(n_samples)])
    
    return X, y


def train_multiclass_lightgbm(X: np.ndarray, y: np.ndarray, 
                             num_boost_round: int = 10,
                             num_classes: int = 3,
                             seed: int = 42) -> Dict:
    """
    å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã§LightGBMã‚’è¨“ç·´
    
    Args:
        X: ç‰¹å¾´é‡è¡Œåˆ—
        y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé…åˆ—
        num_boost_round: ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ•°
        num_classes: ã‚¯ãƒ©ã‚¹æ•°
        seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        
    Returns:
        è¨“ç·´çµæœã®è©³ç´°æƒ…å ±
    """
    lgb_train = lgb.Dataset(X, y)
    
    params = {
        'objective': 'multiclass',
        'metric': 'multi_logloss',
        'num_class': num_classes,
        'num_leaves': 15,
        'min_data_in_leaf': 10,
        'learning_rate': 0.1,
        'feature_fraction': 1.0,
        'verbose': -1,
        'seed': seed,
        'deterministic': True
    }
    
    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§è©³ç´°æƒ…å ±ã‚’åé›†
    eval_results = {}
    
    model = lgb.train(
        params=params,
        train_set=lgb_train,
        num_boost_round=num_boost_round,
        callbacks=[lgb.record_evaluation(eval_results)]
    )
    
    # é€šå¸¸ã®äºˆæ¸¬ï¼ˆãƒãƒ¼ã‚¸ãƒ³ï¼‰
    predictions_margin = model.predict(X, num_iteration=model.best_iteration)
    
    # ç¢ºç‡äºˆæ¸¬
    predictions_proba = model.predict(X, num_iteration=model.best_iteration)
    
    # LightGBMã®multiclassã¯è‡ªå‹•ã§SoftmaxãŒé©ç”¨ã•ã‚Œã‚‹
    
    return {
        'model': model,
        'params': params,
        'predictions_margin': predictions_margin.tolist(),
        'predictions_proba': predictions_proba.tolist(),
        'eval_results': eval_results,
        'num_trees': model.num_trees(),
        'feature_importance': model.feature_importance().tolist()
    }


def test_multiclass_probability_predictions():
    """å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã§ã®ç¢ºç‡äºˆæ¸¬ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("=== å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ç¢ºç‡äºˆæ¸¬ãƒ†ã‚¹ãƒˆ ===")
    
    X, y = create_multiclass_dataset(n_samples=200, n_classes=3, seed=123)
    
    # LightGBMã§è¨“ç·´
    result = train_multiclass_lightgbm(X, y, num_boost_round=5, num_classes=3, seed=123)
    
    predictions_proba = np.array(result['predictions_proba'])
    
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {X.shape}")
    print(f"ã‚¯ãƒ©ã‚¹æ•°: 3")
    print(f"äºˆæ¸¬ç¢ºç‡ã®å½¢çŠ¶: {predictions_proba.shape}")
    
    # ç¢ºç‡ã®æ€§è³ªã‚’ãƒã‚§ãƒƒã‚¯
    for i in range(min(5, len(predictions_proba))):
        proba_sum = np.sum(predictions_proba[i])
        print(f"ã‚µãƒ³ãƒ—ãƒ« {i}: ç¢ºç‡åˆè¨ˆ = {proba_sum:.6f}, äºˆæ¸¬ = {predictions_proba[i]}")
        
        # ç¢ºç‡ã®åˆè¨ˆã¯1ã«è¿‘ã„ã¯ãš
        assert abs(proba_sum - 1.0) < 1e-6, f"ç¢ºç‡ã®åˆè¨ˆãŒ1ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {proba_sum}"
        
        # å…¨ã¦ã®ç¢ºç‡ã¯0ä»¥ä¸Š1ä»¥ä¸‹
        assert np.all(predictions_proba[i] >= 0), f"è² ã®ç¢ºç‡ãŒã‚ã‚Šã¾ã™: {predictions_proba[i]}"
        assert np.all(predictions_proba[i] <= 1), f"1ã‚’è¶…ãˆã‚‹ç¢ºç‡ãŒã‚ã‚Šã¾ã™: {predictions_proba[i]}"
    
    print("âœ… å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ç¢ºç‡äºˆæ¸¬ãƒ†ã‚¹ãƒˆ: PASS")
    return result


def test_softmax_consistency():
    """Softmaxå¤‰æ›ã®ä¸€è²«æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("=== Softmaxå¤‰æ›ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ ===")
    
    X, y = create_multiclass_dataset(n_samples=100, n_classes=4, seed=456)
    
    # LightGBMã§è¨“ç·´
    result = train_multiclass_lightgbm(X, y, num_boost_round=3, num_classes=4, seed=456)
    
    predictions_proba = np.array(result['predictions_proba'])
    
    # æ‰‹å‹•ã§Softmaxã‚’ç¢ºèª
    # LightGBMã®multiclass objectiveã¯å†…éƒ¨ã§Softmaxã‚’é©ç”¨ã™ã‚‹ãŸã‚ã€
    # ç”Ÿã®logitã‚’å–å¾—ã™ã‚‹ã®ã¯å›°é›£ã€‚ä»£ã‚ã‚Šã«ä¸€è²«æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    
    print(f"ç¢ºç‡äºˆæ¸¬ã®çµ±è¨ˆ:")
    print(f"  æœ€å°å€¤: {np.min(predictions_proba):.6f}")
    print(f"  æœ€å¤§å€¤: {np.max(predictions_proba):.6f}")
    print(f"  å¹³å‡å€¤: {np.mean(predictions_proba):.6f}")
    print(f"  è¡Œåˆè¨ˆã®å¹³å‡: {np.mean(np.sum(predictions_proba, axis=1)):.6f}")
    print(f"  è¡Œåˆè¨ˆã®æ¨™æº–åå·®: {np.std(np.sum(predictions_proba, axis=1)):.6f}")
    
    # å…¨ã¦ã®è¡Œã®åˆè¨ˆã¯1ã«è¿‘ã„ã¯ãš
    row_sums = np.sum(predictions_proba, axis=1)
    assert np.allclose(row_sums, 1.0, atol=1e-6), "ç¢ºç‡ã®è¡Œåˆè¨ˆãŒ1ã§ã¯ã‚ã‚Šã¾ã›ã‚“"
    
    print("âœ… Softmaxå¤‰æ›ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ: PASS")
    return result


def test_class_prediction_accuracy():
    """ã‚¯ãƒ©ã‚¹äºˆæ¸¬ç²¾åº¦ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("=== ã‚¯ãƒ©ã‚¹äºˆæ¸¬ç²¾åº¦ãƒ†ã‚¹ãƒˆ ===")
    
    X, y = create_multiclass_dataset(n_samples=400, n_classes=3, seed=789)
    
    # è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    split_idx = int(len(X) * 0.7)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # LightGBMã§è¨“ç·´
    lgb_train = lgb.Dataset(X_train, y_train)
    params = {
        'objective': 'multiclass',
        'metric': 'multi_logloss',
        'num_class': 3,
        'num_leaves': 20,
        'min_data_in_leaf': 5,
        'learning_rate': 0.1,
        'verbose': -1,
        'seed': 789,
        'deterministic': True
    }
    
    model = lgb.train(
        params=params,
        train_set=lgb_train,
        num_boost_round=15
    )
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
    y_pred_proba = model.predict(X_test)
    y_pred_class = np.argmax(y_pred_proba, axis=1)
    
    # ç²¾åº¦è¨ˆç®—
    accuracy = np.mean(y_pred_class == y_test)
    print(f"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆç²¾åº¦: {accuracy:.3f}")
    
    # ã‚¯ãƒ©ã‚¹åˆ¥ã®äºˆæ¸¬ç¢ºç‡çµ±è¨ˆ
    for c in range(3):
        class_mask = y_test == c
        if np.any(class_mask):
            class_proba = y_pred_proba[class_mask, c]
            print(f"ã‚¯ãƒ©ã‚¹ {c}: å¹³å‡ç¢ºç‡ = {np.mean(class_proba):.3f}, "
                  f"æ­£è§£æ™‚ã®å¹³å‡ç¢ºç‡ = {np.mean(y_pred_proba[class_mask & (y_pred_class == c), c]):.3f}")
    
    # æœ€ä½é™ã®ç²¾åº¦ã‚’æœŸå¾…
    assert accuracy > 0.3, f"äºˆæ¸¬ç²¾åº¦ãŒä½ã™ãã¾ã™: {accuracy}"
    
    print("âœ… ã‚¯ãƒ©ã‚¹äºˆæ¸¬ç²¾åº¦ãƒ†ã‚¹ãƒˆ: PASS")
    return {
        'accuracy': accuracy,
        'y_pred_proba': y_pred_proba.tolist(),
        'y_pred_class': y_pred_class.tolist(),
        'y_test': y_test.tolist()
    }


def create_go_test_data():
    """Goå®Ÿè£…ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    print("=== Goå®Ÿè£…ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ ===")
    
    # å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©³ç´°ãªæ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    X, y = create_multiclass_dataset(n_samples=80, n_features=4, n_classes=3, seed=999)
    
    # MulticlassLogLossè¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    result = train_multiclass_lightgbm(X, y, num_boost_round=5, num_classes=3, seed=999)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
    test_data = {
        'dataset': {
            'X': X.tolist(),
            'y': y.tolist(),
            'n_samples': int(X.shape[0]),
            'n_features': int(X.shape[1]),
            'n_classes': 3
        },
        'training_params': {
            'objective': 'multiclass',
            'num_class': 3,
            'num_leaves': 15,
            'min_data_in_leaf': 10,
            'learning_rate': 0.1,
            'num_boost_round': 5,
            'seed': 999,
            'verbose': -1
        },
        'lightgbm_result': {
            'predictions_proba': result['predictions_proba'],
            'num_trees': result['num_trees'],
            'feature_importance': result['feature_importance']
        },
        'expected_behavior': {
            'probability_sum_per_sample': 1.0,
            'probability_range': [0.0, 1.0],
            'softmax_transformation': 'Applied automatically for multiclass objective'
        },
        'test_info': {
            'description': 'PredictProba verification data for Go implementation',
            'focus': 'MulticlassLogLoss integration and Softmax consistency',
            'tolerance': 1e-5
        }
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    os.makedirs('testdata', exist_ok=True)
    with open('testdata/predictproba_verification_data.json', 'w') as f:
        json.dump(test_data, f, indent=2)
    
    print(f"âœ… Goå®Ÿè£…ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: testdata/predictproba_verification_data.json")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {X.shape}")
    print(f"ã‚¯ãƒ©ã‚¹æ•°: 3")
    
    # äºˆæ¸¬ç¢ºç‡ã®çµ±è¨ˆæƒ…å ±
    predictions_proba = np.array(result['predictions_proba'])
    print(f"ç¢ºç‡äºˆæ¸¬çµ±è¨ˆ:")
    print(f"  å½¢çŠ¶: {predictions_proba.shape}")
    print(f"  ç¢ºç‡ç¯„å›²: [{np.min(predictions_proba):.3f}, {np.max(predictions_proba):.3f}]")
    print(f"  è¡Œåˆè¨ˆã®ç¯„å›²: [{np.min(np.sum(predictions_proba, axis=1)):.6f}, {np.max(np.sum(predictions_proba, axis=1)):.6f}]")
    
    return test_data


def test_edge_cases():
    """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆ"""
    print("=== ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ ===")
    
    # æ¥µç«¯ã«åã£ãŸã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
    X, y = create_multiclass_dataset(n_samples=150, n_classes=3, seed=555)
    
    # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’äººå·¥çš„ã«åã‚‰ã›ã‚‹
    class_counts = np.bincount(y)
    print(f"å…ƒã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {class_counts}")
    
    # ã‚¯ãƒ©ã‚¹0ã‚’å¤šæ•°æ´¾ã«ã™ã‚‹
    majority_mask = y == 0
    minority_indices = np.where(~majority_mask)[0]
    
    # ä¸€éƒ¨ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚¯ãƒ©ã‚¹0ã«å¤‰æ›´
    change_indices = minority_indices[:len(minority_indices)//2]
    y[change_indices] = 0
    
    new_class_counts = np.bincount(y)
    print(f"èª¿æ•´å¾Œã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {new_class_counts}")
    
    # LightGBMã§è¨“ç·´
    result = train_multiclass_lightgbm(X, y, num_boost_round=8, num_classes=3, seed=555)
    
    predictions_proba = np.array(result['predictions_proba'])
    
    # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ä¸‹ã§ã‚‚ç¢ºç‡ã®æ€§è³ªãŒä¿ãŸã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    row_sums = np.sum(predictions_proba, axis=1)
    assert np.allclose(row_sums, 1.0, atol=1e-6), "ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã§ç¢ºç‡åˆè¨ˆãŒ1ã§ã¯ã‚ã‚Šã¾ã›ã‚“"
    
    # å¤šæ•°æ´¾ã‚¯ãƒ©ã‚¹ã®å¹³å‡ç¢ºç‡ãŒé«˜ã„ã“ã¨ã‚’ç¢ºèª
    majority_class_proba = np.mean(predictions_proba[:, 0])
    minority_class_proba = np.mean(predictions_proba[:, 1:])
    print(f"å¤šæ•°æ´¾ã‚¯ãƒ©ã‚¹(0)ã®å¹³å‡ç¢ºç‡: {majority_class_proba:.3f}")
    print(f"å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®å¹³å‡ç¢ºç‡: {minority_class_proba:.3f}")
    
    print("âœ… ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ: PASS")
    return result


def main():
    """å…¨ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    print("=== PredictProbaæ¤œè¨¼ãƒ†ã‚¹ãƒˆé–‹å§‹ ===\n")
    
    try:
        # 1. å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ç¢ºç‡äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
        print("1. å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ç¢ºç‡äºˆæ¸¬ãƒ†ã‚¹ãƒˆ")
        test_multiclass_probability_predictions()
        print()
        
        # 2. Softmaxå¤‰æ›ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ  
        print("2. Softmaxå¤‰æ›ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ")
        test_softmax_consistency()
        print()
        
        # 3. ã‚¯ãƒ©ã‚¹äºˆæ¸¬ç²¾åº¦ãƒ†ã‚¹ãƒˆ
        print("3. ã‚¯ãƒ©ã‚¹äºˆæ¸¬ç²¾åº¦ãƒ†ã‚¹ãƒˆ")
        test_class_prediction_accuracy()
        print()
        
        # 4. ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
        print("4. ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ")
        test_edge_cases()
        print()
        
        # 5. Goå®Ÿè£…ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        print("5. Goå®Ÿè£…ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ")
        create_go_test_data()
        print()
        
        print("ğŸ‰ å…¨PredictProbaæ¤œè¨¼ãƒ†ã‚¹ãƒˆå®Œäº†: SUCCESS")
        
    except Exception as e:
        print(f"âŒ PredictProbaæ¤œè¨¼ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        raise


if __name__ == "__main__":
    main()
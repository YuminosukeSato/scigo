//go:build parity

package linear_model

import (
	"encoding/json"
	"os"
	"testing"

	"gonum.org/v1/gonum/mat"
)

type logRegGolden struct {
	Meta      map[string]any `json:"meta"`
	Coef      [][]float64    `json:"coef_"`
	Intercept []float64      `json:"intercept_"`
	Classes   []int          `json:"classes_"`
	NIter     []int          `json:"n_iter_"`
	XTest     [][]float64    `json:"X_test"`
	YTest     []int          `json:"y_test"`
	Pred      []int          `json:"pred"`
	Proba     [][]float64    `json:"proba"`
}

func loadLogRegGolden(t *testing.T, path string) *logRegGolden {
	t.Helper()
	b, err := os.ReadFile(path)
	if err != nil {
		t.Skipf("golden not found: %v", err)
	}
	var g logRegGolden
	if err := json.Unmarshal(b, &g); err != nil {
		t.Fatalf("unmarshal golden: %v", err)
	}
	return &g
}

func nearlyEqual(a, b, rtol, atol float64) bool {
	if a == b {
		return true
	}
	da := a - b
	if da < 0 {
		da = -da
	}
	if da <= atol {
		return true
	}
	if a < 0 {
		a = -a
	}
	if b < 0 {
		b = -b
	}
	if da <= atol+rtol*max(a, b) {
		return true
	}
	return false
}

func max(a, b float64) float64 {
	if a > b {
		return a
	}
	return b
}

// TestLogistic_Parity_Case1 compares against a golden generated by scikit-learn.
// Note: current implementation uses simple gradient descent, not lbfgs.
// After introducing lbfgs/newton-cg, tighten the error tolerances.
func TestLogistic_Parity_Case1(t *testing.T) {
	g := loadLogRegGolden(t, "tests/golden/logreg_case1.json")

	if len(g.XTest) == 0 {
		t.Skip("empty golden X_test")
	}

	nRows := len(g.XTest)
	nCols := len(g.XTest[0])
	X := mat.NewDense(nRows, nCols, nil)
	for i := 0; i < nRows; i++ {
		for j := 0; j < nCols; j++ {
			X.Set(i, j, g.XTest[i][j])
		}
	}
	y := mat.NewDense(len(g.YTest), 1, nil)
	for i := range g.YTest {
		y.Set(i, 0, float64(g.YTest[i]))
	}

	clf := NewLogisticRegression(
		WithLRPenalty("l2"),
		WithLRC(0.7),
		WithLRSolver("lbfgs"), // 実装後に有効化
		WithLRMaxIter(1000),
		WithLRTol(1e-6),
		WithLRRandomState(0),
	)
	if err := clf.Fit(X, y); err != nil {
		t.Fatalf("fit: %v", err)
	}

	// 係数と切片の行列サイズのみ先に確認
	if len(clf.coef_) != len(g.Coef) {
		t.Fatalf("coef rows mismatch: got %d want %d", len(clf.coef_), len(g.Coef))
	}
	if len(clf.coef_) > 0 && len(clf.coef_[0]) != len(g.Coef[0]) {
		t.Fatalf("coef cols mismatch: got %d want %d", len(clf.coef_[0]), len(g.Coef[0]))
	}
	if len(clf.intercept_) != len(g.Intercept) {
		t.Fatalf("intercept len mismatch: got %d want %d", len(clf.intercept_), len(g.Intercept))
	}

	// 予測確率の形状一致と緩い近似チェック
	proba, err := clf.PredictProba(X)
	if err != nil {
		t.Fatalf("predict_proba: %v", err)
	}
	if proba.RawMatrix().Rows != len(g.Proba) || proba.RawMatrix().Cols != len(g.Proba[0]) {
		t.Fatalf("proba shape mismatch: got (%d,%d) want (%d,%d)", proba.RawMatrix().Rows, proba.RawMatrix().Cols, len(g.Proba), len(g.Proba[0]))
	}

	// 現段階では緩い誤差で確認
	rtol, atol := 1e-3, 1e-4
	for i := 0; i < proba.RawMatrix().Rows; i++ {
		for j := 0; j < proba.RawMatrix().Cols; j++ {
			if !nearlyEqual(proba.At(i, j), g.Proba[i][j], rtol, atol) {
				// 厳密一致でない場合は情報を提供して失敗
				t.Fatalf("proba mismatch at (%d,%d): got=%g want=%g", i, j, proba.At(i, j), g.Proba[i][j])
			}
		}
	}
}
